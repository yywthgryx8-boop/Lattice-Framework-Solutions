This repo contains a set of small Python demos that show how one can steer LLM behavior at inference time 
using constraints (invariants), mode routing, and simple feedback—without changing model weights. Each folder 
focuses on a single idea. Sentinel is a guard; the feedback loop demonstrates an entrainment layer that is 
not part of the LLM; and Social Scientist mode represents a research-oriented workflow. The code is meant to be 
easy to read and run, and it is written to expose the underlying system and gating logic rather than to showcase 
a specific model. This kind of control layer has high utility for professionals working on LLM tooling, safety checks, 
evaluation, or human–AI workflows.

One-sentence demo summaries:

SentinelOS Demo: A lightweight guard that scores outputs and returns allow, warn, or block decisions without retraining.

Entrainment Feedback Loop Demo: A simple reward/punishment feedback layer that nudges mode selection over time using 
invariant weights rather than hard rules.

Social Scientist Demo: A runnable scaffold that routes research questions and data through qualitative lenses and 
sketches a mixed-methods study design.
